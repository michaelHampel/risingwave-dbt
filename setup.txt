// risingwave setup - starts risingwave, postgres, redpanda, grafana, prometheus
run docker compose up -d in risingwave/docker folder

//create redpanda tokens
rpk topic create rw-test
rpk topic create rw-sink
rpk token list

// connect to redpanda with external kafkaTopic.schema
cd /opt/homebrew/Cellar/kafka/3.7.0/bin
kafka-topics --bootstrap-server localhost:9092 --describe --topic rw-test

// python and dbt-risingwave adapter setup
cd dbt
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

cd risingwave

// dbt project init
# create dbt project for risisingwave (running in docker)
# input parameters to set up a profile to connect to the local RisingWave:
# - host: 127.0.0.1
# - port: 4566
# - user: root
dbt init risingwave_demo
# check that the adapter is installed successfully
cd risingwave_demo
dbt debug --profiles-dir .

// content of profiles.yml

risingwave_demo:
  outputs:
    dev:
      dbname: dev
      host: 127.0.0.1
      password: ''
      port: 4566
      schema: public
      threads: 1
      type: risingwave
      user: root
  target: dev


// dbt project init
# create dbt project for postgres (running in docker)
# input parameters to set up a profile to connect to the local RisingWave:
# - host: localhost
# - port: 5432
# - user: postgres
dbt init postgres_demo
# check that the adapter is installed successfully
cd postgres_demo
dbt debug 

// content of profiles.yml

postgres_demo:
  outputs:
    dev:
      dbname: postgres
      host: localhost
      password: ''
      port: 5432
      schema: public
      threads: 1
      type: postgres
      user: postgres
  target: dev


// Create raw_user table in postgres and add data
dbt sink

// Create users view from raw_user table
dbt run

// or all-in-one
dbt build

// connect to postgres and check if table, view has been created and data persisted
psql -h localhost -p 5432 -U postgres

\dv //show all views
\dt // show all tables

// run data generator (sends data to rw-test topic)
python data-generator.py 

// for Jupyter Notebook
// create kernel
python3 -m ipykernel install --user --name=rw_kernel


// create grafana dashboard 
docker exec -it redpanda-0 rpk generate grafana-dashboard \
--datasource prometheus \
--metrics-endpoint otel-collector:8889/metrics > redpanda-dashboard-otel.json

rpk generate grafana-dashboard \
  --datasource prometheus \
  --metrics-endpoint redpanda-0:9644/public_metrics > redpanda-dashboard.json

https://github.com/Aiven-Open/karapace?tab=readme-ov-file

curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  --data @smartMeter_schema.json \
  http://localhost:8083/subjects/smartMeter-incoming-value/versions

curl -X GET http://localhost:8083/subjects 
curl -X DELETE http://localhost:8083/subjects/test-key
curl "http://localhost:8084/topics"


-H "Content-Type: application/vnd.schemaregistry.v1+json"

  echo '{
  "owner": {
      "type": "object",
      "properties": {
        "id": {
          "type": "string"
        }
      },
      "required": [
        "id"
      ]
  },
  "required": [
    "owner" 
  ],
  "additionalProperties" : false
}' | \
jq '. | {schema: tojson, schemaType: "JSON"}' | \
curl -X POST "http://localhost:8083/subjects/test1-key-json-schema/versions" -H "Content-Type:application/json"  -d @- 

{"current":{"L1":{"centiAmpere":0},"L2":{"centiAmpere":0},"L3":{"centiAmpere":0}},"device":{"deviceId":{"value":"ECUC012A3101E2.cuculus.net"}},"energy":{"consumption":{"wattHours":140118},"feedIn":{"wattHours":0}},"id":{"id":"11f82b93-d80d-442d-a2fc-d148422e3700"},"meter":{"meterId":{"value":"1KFM0200001600"},"systemTitle":{"data":"4b464d1020000640"}},"owner":{"id":"59c3d063-d6d8-4d07-b1f5-e3a21d884c12"},"power":{"draw":{"watt":0},"feed":{"watt":0}},"readingFrom":"2024-06-13T22:07:55Z","receivedAt":"2024-06-13T20:07:58Z","voltage":{"L1":{"deciVolt":2369},"L2":{"deciVolt":0},"L3":{"deciVolt":0}}}
{"device":{"deviceId":{"value":"ECUC012A3101E2.cuculus.net"}},"energy":{"consumption":{"wattHours":140118},"feedIn":{"wattHours":0}},"id":{"id":"11f82b93-d80d-442d-a2fc-d148422e3700"},"meter":{"meterId":{"value":"1KFM0200001600"},"systemTitle":{"data":"4b464d1020000640"}},"owner":{"id":"59c3d063-d6d8-4d07-b1f5-e3a21d884c12"},"power":{"draw":{"watt":0},"feed":{"watt":0}},"readingFrom":"2024-06-13T22:07:55Z","receivedAt":"2024-06-13T20:07:58Z","voltage":{"L1":{"deciVolt":2369},"L2":{"deciVolt":0},"L3":{"deciVolt":0}}}


kafka-console-producer \
  --bootstrap-server localhost:9092 \
  --topic smartMeter-incoming  \
  --property schema.registry.url=http://localhost:8083 \
  --property value.schema.id=3

 value.schema="$(curl http://localhost:8081/schemas/ids/419 | jq -r .schema)"

 conn.execute(f"""COPY ( 
                        SELECT owner, device, energy, readingFrom
                        FROM read_parquet('s3://{bucket_name}/{live_prefix}/**/*.parquet', filename = true) 
                        WHERE year = {year}
                        AND month = {month} 
                        AND day = {day}
                        AND owner.id = '{owner_id}'
                        ORDER BY readingFrom ASC
                    ) TO 's3://{bucket_name}/results-snappy.parquet' (FORMAT 'parquet');""")


  command: 
      - bash 
      - -c 
      - |
        #
        echo "Installing connector plugins"
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:latest
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run & 
        #
        echo "Waiting for Kafka Connect to start listening on localhost ‚è≥"
        while : ; do
          curl_status=$$(curl -s -o /dev/null -w %{http_code} http://localhost:8083/connectors)
          echo -e $$(date) " Kafka Connect listener HTTP state: " $$curl_status " (waiting for 200)"
          if [ $$curl_status -eq 200 ] ; then
            break
          fi
          sleep 5 
        done
        curl -X PUT \
        -H 'Content-Type: application/json' \
        -H 'Accept: application/json' http://localhost:8083/connectors/S3SinkConnector/config \
        -d '{
        "connector.class": "io.confluent.connect.s3.S3SinkConnector",
        "storage.class": "io.confluent.connect.s3.storage.S3Storage",
        "s3.region": "eu-west-1",
        "s3.bucket.name": "enox-bucket",
        "flush.size": "150",
        "schema.compatibility": "NONE",
        "tasks.max": "1",
        "topics": "smartMeter-incoming",
        "store.url": "http://minio-0:9301",
        "key.converter.schemas.enable": "false",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
        "partitioner.class": "io.confluent.connect.storage.partitioner.DefaultPartitioner",
        "value.converter.schemas.enable": "false",
        "value.converter": "io.confluent.connect.json.JsonSchemaConverter",
        "value.converter.schema.registry.url": "http://karapace-registry:8085"
        }'
        sleep infinity

"path.format" : "'\'year\''=YYYY/'\'month\''=MM/'\'day\''=dd/'\'hour\''=HH",
"partition.duration.ms": "600000",
"rotate.schedule.interval.ms": "600000",

INFO S3SinkConnectorConfig values: 
2024-08-12 15:02:03     allow.optional.map.keys = false
2024-08-12 15:02:03     avro.codec = null
2024-08-12 15:02:03     aws.access.key.id = 
2024-08-12 15:02:03     aws.secret.access.key = [hidden]
2024-08-12 15:02:03     behavior.on.null.values = fail
2024-08-12 15:02:03     connect.meta.data = true
2024-08-12 15:02:03     enhanced.avro.schema.support = true
2024-08-12 15:02:03     filename.offset.zero.pad.width = 10
2024-08-12 15:02:03     flush.size = 150
2024-08-12 15:02:03     format.bytearray.extension = .bin
2024-08-12 15:02:03     format.bytearray.separator = null
2024-08-12 15:02:03     format.class = class io.confluent.connect.s3.format.parquet.ParquetFormat
2024-08-12 15:02:03     headers.format.class = class io.confluent.connect.s3.format.avro.AvroFormat
2024-08-12 15:02:03     json.decimal.format = BASE64
2024-08-12 15:02:03     keys.format.class = class io.confluent.connect.s3.format.avro.AvroFormat
2024-08-12 15:02:03     parquet.codec = snappy
2024-08-12 15:02:03     retry.backoff.ms = 5000
2024-08-12 15:02:03     rotate.interval.ms = -1
2024-08-12 15:02:03     rotate.schedule.interval.ms = -1
2024-08-12 15:02:03     s3.acl.canned = null
2024-08-12 15:02:03     s3.bucket.name = enox
2024-08-12 15:02:03     s3.compression.level = -1
2024-08-12 15:02:03     s3.compression.type = none
2024-08-12 15:02:03     s3.credentials.provider.class = class com.amazonaws.auth.DefaultAWSCredentialsProviderChain
2024-08-12 15:02:03     s3.elastic.buffer.enable = false
2024-08-12 15:02:03     s3.elastic.buffer.init.capacity = 131072
2024-08-12 15:02:03     s3.http.send.expect.continue = true
2024-08-12 15:02:03     s3.object.behavior.on.tagging.error = ignore
2024-08-12 15:02:03     s3.object.tagging = false
2024-08-12 15:02:03     s3.part.retries = 3
2024-08-12 15:02:03     s3.part.size = 26214400
2024-08-12 15:02:03     s3.path.style.access.enabled = true
2024-08-12 15:02:03     s3.proxy.password = [hidden]
2024-08-12 15:02:03     s3.proxy.url = 
2024-08-12 15:02:03     s3.proxy.user = null
2024-08-12 15:02:03     s3.region = eu-west-1
2024-08-12 15:02:03     s3.retry.backoff.ms = 200
2024-08-12 15:02:03     s3.schema.partition.affix.type = NONE
2024-08-12 15:02:03     s3.sse.customer.key = [hidden]
2024-08-12 15:02:03     s3.sse.kms.key.id = 
2024-08-12 15:02:03     s3.ssea.name = 
2024-08-12 15:02:03     s3.wan.mode = false
2024-08-12 15:02:03     schema.compatibility = NONE
2024-08-12 15:02:03     schemas.cache.config = 1000
2024-08-12 15:02:03     shutdown.timeout.ms = 3000
2024-08-12 15:02:03     store.kafka.headers = false
2024-08-12 15:02:03     store.kafka.keys = false
2024-08-12 15:02:03     tombstone.encoded.partition = tombstone
2024-08-12 15:02:03  (io.confluent.connect.s3.S3SinkConnectorConfig)
2024-08-12 15:02:03 [2024-08-12 13:02:03,651] INFO StorageCommonConfig values: 
2024-08-12 15:02:03     directory.delim = /
2024-08-12 15:02:03     file.delim = +
2024-08-12 15:02:03     storage.class = class io.confluent.connect.s3.storage.S3Storage
2024-08-12 15:02:03     store.url = http://minio-0:9301
2024-08-12 15:02:03     topics.dir = live
2024-08-12 15:02:03  (io.confluent.connect.storage.common.StorageCommonConfig)
2024-08-12 15:02:03 [2024-08-12 13:02:03,651] INFO PartitionerConfig values: 
2024-08-12 15:02:03     locale = de-DE
2024-08-12 15:02:03     partition.duration.ms = -1
2024-08-12 15:02:03     partition.field.name = []
2024-08-12 15:02:03     partitioner.class = class io.confluent.connect.storage.partitioner.DailyPartitioner
2024-08-12 15:02:03     path.format = 
2024-08-12 15:02:03     timestamp.extractor = RecordField
2024-08-12 15:02:03     timestamp.field = readingFrom
2024-08-12 15:02:03     timezone = UTC
2024-08-12 15:02:03  (io.confluent.connect.storage.partitioner.PartitionerConfig)
2024-08-12 15:02:03 [2024-08-12 13:02:03,652] INFO Creating S3 client. (io.confluent.connect.s3.storage.S3Storage)
2024-08-12 15:02:03 [2024-08-12 13:02:03,675] INFO Created a retry policy for the connector (io.confluent.connect.s3.storage.S3Storage)
2024-08-12 15:02:03 [2024-08-12 13:02:03,678] INFO Returning new credentials provider based on the configured credentials provider class (io.confluent.connect.s3.storage.S3Storage)
2024-08-12 15:02:03 [2024-08-12 13:02:03,678] INFO S3 client created (io.confluent.connect.s3.storage.S3Storage)
2024-08-12 15:02:03 [2024-08-12 13:02:03,824] INFO AvroDataConfig values: 
2024-08-12 15:02:03     allow.optional.map.keys = false
2024-08-12 15:02:03     connect.meta.data = true
2024-08-12 15:02:03     discard.type.doc.default = false
2024-08-12 15:02:03     enhanced.avro.schema.support = true
2024-08-12 15:02:03     flatten.singleton.unions = true
2024-08-12 15:02:03     generalized.sum.type.support = false
2024-08-12 15:02:03     ignore.default.for.nullables = false
2024-08-12 15:02:03     schemas.cache.config = 1000
2024-08-12 15:02:03     scrub.invalid.names = false


Calculate the average temperature and humidity for each device in a time window

CREATE MATERIALIZED VIEW device_avg_mv AS
SELECT device_id, 
AVG(temperature) AS avg_temperature,
AVG(humidity) AS avg_humidity,
window_start, window_end
FROM TUMBLE (iot_sensor_data, ts, INTERVAL '1 MINUTES')
GROUP BY device_id,window_start, window_end;
Counts device status as either Normal or Abnormal within a time window

CREATE MATERIALIZED VIEW device_status_count_mv AS
SELECT  device_id,
COUNT(CASE WHEN device_status = 'normal' THEN 1 ELSE NULL END) AS normal_count,
COUNT(CASE WHEN device_status = 'abnormal' THEN 1 ELSE NULL END) AS abnormal_count, 
window_start, window_end
FROM TUMBLE (iot_sensor_data, ts, INTERVAL '1 MINUTES')
GROUP BY device_id, window_start, window_end;
Calculate the max and min temperature and humidity for each device in a time window

CREATE MATERIALIZED VIEW device_max_min_mv AS
SELECT device_id, 
MAX(temperature) AS max_temperature,
MAX(humidity) AS max_humidity,
MIN(temperature) AS min_temperature,
MIN(humidity) AS min_humidity,
window_start, window_end
FROM TUMBLE (iot_sensor_data, ts, INTERVAL '1 MINUTES')
GROUP BY device_id,window_start, window_end;